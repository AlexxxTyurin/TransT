{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JgYqoJ7m9xqj",
    "colab_type": "code",
    "outputId": "732fe243-c6de-4fa7-ecea-a40e98c9a9ee",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.587295829361E12,
     "user_tz": -180.0,
     "elapsed": 957.0,
     "user": {
      "displayName": "Александр Тюрин",
      "photoUrl": "",
      "userId": "16220637881071339753"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZRgUDhLg91tq",
    "colab_type": "code",
    "outputId": "0e447c5f-1060-4285-c1b3-c1b04839fd65",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.587295835502E12,
     "user_tz": -180.0,
     "elapsed": 7080.0,
     "user": {
      "displayName": "Александр Тюрин",
      "photoUrl": "",
      "userId": "16220637881071339753"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dask import delayed\n",
    "from torch.autograd import Variable\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import tensorflow as tf \n",
    "import os\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "import operator\n",
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "x9zUojDPM0UY",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "train_path = '/content/drive/My Drive/TransT/train.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "GkEahfOrrZ3x",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "types_path = '/content/drive/My Drive/TransT/newType.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "ezWdoIiUf7Tu",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "entities_to_index_path = \"/content/drive/My Drive/TransT/entities_to_index.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "w0i5vDV_f7dl",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "relations_to_index_path = \"/content/drive/My Drive/TransT/relations_to_index.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "IvE8btfY91wP",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "class TrainDataset:\n",
    "    \"\"\"\n",
    "    Класс нашего датасета. С помошью него для модели TransT реализован Negative\n",
    "    Sampling. \n",
    "    \"\"\"\n",
    "    def __init__(self, train_path, types_path, entities_to_index_path, relations_to_index_path):\n",
    "        \"\"\"\n",
    "        train_path - путь до файла, к котором хранятся данные для обучения \n",
    "        train_path - путь до файла, к котором хранятся данные о типах для каждого entity\n",
    "        entities_to_index_path - путь до файла, где хранится то, как переводить ссылки на entity в целочисленные индексы\n",
    "        relations_to_index_path - путь до файла, где хранится то, как переводить ссылки на relation в целочисленные индексы\n",
    "        \"\"\"\n",
    "        self.train_path = train_path\n",
    "        self.types_path = types_path\n",
    "        self.entities_to_index_path = entities_to_index_path\n",
    "        self.relations_to_index_path = relations_to_index_path\n",
    "        \n",
    "        self.entities_to_index, self.relations_to_index = self.create_converters()\n",
    "\n",
    "        self.head_rel_to_tail = {}\n",
    "        self.tail_rel_to_head = {}\n",
    "        self.head_tail_to_rel = {}\n",
    "\n",
    "        self.pos_triplets = self.generate_positive_triplets()\n",
    "        self.neg_triplets = self.generate_negative_triplets()\n",
    "\n",
    "        self.types_dict = self.generate_types_dict()\n",
    "\n",
    "    def create_converters(self):\n",
    "        \"\"\"\n",
    "        Считываем файлы и передаем их в словари.\n",
    "        Выводит: кортеж из 2 словарей(entities_to_index, relations_to_index)\n",
    "        entities_to_index: словарь, ключами которого являются ссылки на entity, а значениями - целочисленный индекс\n",
    "        relations_to_index: словарь, ключами которого являются ссылки на relation, а значениями - целочисленный индекс\n",
    "        \"\"\"\n",
    "        with open(self.entities_to_index_path, 'r') as f:\n",
    "            for el in f:\n",
    "                entities_to_index = json.loads(json.loads(el))\n",
    "\n",
    "        with open(self.relations_to_index_path, 'r') as f:\n",
    "            for el in f:\n",
    "                relations_to_index = json.loads(json.loads(el))\n",
    "        \n",
    "        return entities_to_index, relations_to_index\n",
    "\n",
    "    def generate_positive_triplets(self):\n",
    "        \"\"\"\n",
    "        Считываем файл и группируем его содержимое по триплетам. \n",
    "        Выводит: np.ndarray размером (n, 3), в котором каждая строка отражает связь. Каждая строка состоит из head, rel, tail. \n",
    "\n",
    "        Кроме того, мы заполняем self.head_rel_to_tail, tail_rel_to_head и head_tail_to_rel для того, \n",
    "        чтобы понимать какие ссылки были для сочетаний head и rel, tail и rel ну и head и tail соответственно. \n",
    "        Это нужно для того, чтобы нормально проводить Negative Sampling. Если выбирать значения для Negative Sampling просто \n",
    "        на рандом, то есть вероятность, что попадется рандомное число, которое на самом деле не Negative. Для избежания \n",
    "        сего казуса мы и заполняем эти 3 словаря словарей. \n",
    "        \"\"\"\n",
    "        t = []\n",
    "\n",
    "        with open(self.train_path, 'r') as f:\n",
    "            data = f.readlines()\n",
    "\n",
    "            for el in tqdm(data):\n",
    "                head, rel, tail = [str(a) for a in el.split()]\n",
    "                head = self.entities_to_index[head]\n",
    "                rel = self.relations_to_index[rel]\n",
    "                tail = self.entities_to_index[tail]\n",
    "\n",
    "                t.append([head, rel, tail])\n",
    "\n",
    "\n",
    "                # Fill the heads, rels and tails into the dictionary of dictionaries head_rel_to_tail\n",
    "                if head not in self.head_rel_to_tail.keys():\n",
    "                    self.head_rel_to_tail[head] = {rel: [tail]}\n",
    "                else:\n",
    "                    if rel not in self.head_rel_to_tail[head].keys():\n",
    "                        self.head_rel_to_tail[head][rel] = [tail]\n",
    "                    else:\n",
    "                        self.head_rel_to_tail[head][rel].append(tail)\n",
    "                        \n",
    "                # Fill the tails, rels and heads into the dictionary of dictionaries tail_rel_to_head\n",
    "                if tail not in self.tail_rel_to_head.keys():\n",
    "                    self.tail_rel_to_head[tail] = {rel: [head]}\n",
    "                else:\n",
    "                    if rel not in self.tail_rel_to_head[tail]:\n",
    "                        self.tail_rel_to_head[tail][rel] = [head]\n",
    "                    else:\n",
    "                        self.tail_rel_to_head[tail][rel].append(head)\n",
    "\n",
    "                # Fill the heads, tails and rels into the dictionary of dictionaries head_tail_to_rel\n",
    "                if head not in self.head_tail_to_rel.keys():\n",
    "                    self.head_tail_to_rel[head] = {tail: [rel]}\n",
    "                else:\n",
    "                    if tail not in self.head_tail_to_rel[head].keys():\n",
    "                        self.head_tail_to_rel[head][tail] = [rel]\n",
    "                    else:\n",
    "                        self.head_tail_to_rel[head][tail].append(rel)\n",
    "\n",
    "        pos_triplets = np.array(t)\n",
    "\n",
    "        self.num_ent = len(self.entities_to_index)\n",
    "        self.num_rel = len(self.relations_to_index)               \n",
    "\n",
    "        \n",
    "        return pos_triplets      \n",
    "\n",
    "\n",
    "    def generate_negative_triplets(self):\n",
    "        \"\"\"\n",
    "        Создаем Negative Samples. \n",
    "        Выводит: np.ndarray размером (n, 3, 3). Для каждой строки из self.pos_triplets формируется целых 3 Negative Samples, \n",
    "        поэтому размер не (n, 3).  \n",
    "        \"\"\"\n",
    "        n = []\n",
    "\n",
    "        for i in tqdm(range(self.pos_triplets.shape[0])):\n",
    "            head, rel, tail = self.pos_triplets[i]\n",
    "\n",
    "            neg_head = np.random.randint(0, self.num_ent)\n",
    "            neg_tail = np.random.randint(0, self.num_ent)\n",
    "            neg_rel = np.random.randint(0, self.num_rel)\n",
    "\n",
    "            while neg_tail in self.head_rel_to_tail[head][rel] or neg_tail == tail:\n",
    "                neg_tail = np.random.randint(0, self.num_ent)\n",
    "            \n",
    "            while neg_head in self.tail_rel_to_head[tail][rel] or neg_head == head:\n",
    "                neg_head = np.random.randint(0, self.num_ent)\n",
    "\n",
    "            while neg_rel in self.head_tail_to_rel[head][tail] or neg_rel == rel:\n",
    "                neg_rel = np.random.randint(0, self.num_rel)\n",
    "\n",
    "            n.append([[neg_head, rel, tail], [head, neg_rel, tail], [head, rel, neg_tail]])\n",
    "\n",
    "        return np.array(n)\n",
    "\n",
    "    def generate_types_dict(self):\n",
    "        \"\"\"\n",
    "        Создаем словарь, ключами котрого являются целочисленные представления ссылок на entity, а значениями - списки типов для этих entity\n",
    "        \"\"\"\n",
    "        d = {}\n",
    "        with open(self.types_path, 'r') as f:\n",
    "            data = f.readlines()\n",
    "            for el in tqdm(data):\n",
    "                d[self.entities_to_index[el.split()[0]]] = el.split()[1:]\n",
    "\n",
    "        return d\n",
    "\n",
    "    def generate_types_sets(self, p=0.05):\n",
    "        \"\"\"\n",
    "        Для расчета prior probability нам нужно знать, какме типы наиболее часто встречаются в head или tail для определенного rel. \n",
    "        Именно для этого мы и создаем types_in_head и types_in_tail. \n",
    "        types_in_head, types_in_tail: словари словарей, в которых хранятся частоты вхождений определенных типов в head или tail  для определенных отношений. \n",
    "        \"\"\"\n",
    "        types_in_head =  {}\n",
    "        types_in_tail = {}\n",
    "\n",
    "        for rel in tqdm(self.relations_to_index.values()):\n",
    "            t1 = []\n",
    "            t2 = []\n",
    "            data = self.pos_triplets[self.pos_triplets[:, 1] == rel]\n",
    "            for el in data:\n",
    "                for type_ in self.types_dict[el[0]]:\n",
    "                    t1.append(type_)\n",
    "\n",
    "                for type_ in self.types_dict[el[2]]:\n",
    "                    t2.append(type_)\n",
    "\n",
    "            c_head = Counter(t1)\n",
    "            c_tail = Counter(t2)\n",
    "\n",
    "            num_head = len(t1)\n",
    "            num_tail = len(t2)\n",
    "\n",
    "            types_in_head[rel] = {k: v / num_head for k, v in c_head.items() if v / num_head > p}\n",
    "            types_in_tail[rel] = {k: v / num_tail for k, v in c_tail.items() if v / num_tail > p}\n",
    "\n",
    "\n",
    "        return types_in_head, types_in_tail\n",
    "\n",
    "                \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.pos_triplets[index, :], self.neg_triplets[index, :, :]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.pos_triplets.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Lrx-tdxf91y0",
    "colab_type": "code",
    "outputId": "ff6ec4d8-5ede-4ddf-f5ce-0c83425b9fd5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.587295855712E12,
     "user_tz": -180.0,
     "elapsed": 27263.0,
     "user": {
      "displayName": "Александр Тюрин",
      "photoUrl": "",
      "userId": "16220637881071339753"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68.0
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 483142/483142 [00:04<00:00, 117381.63it/s]\n",
      "100%|██████████| 483142/483142 [00:14<00:00, 33738.54it/s]\n",
      "100%|██████████| 14951/14951 [00:00<00:00, 253049.84it/s]\n"
     ]
    }
   ],
   "source": [
    "a = TrainDataset(train_path, types_path, entities_to_index_path, relations_to_index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "QxFn0JyNutoS",
    "colab_type": "code",
    "outputId": "128bea51-a060-40ac-e978-bf39834f436a",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.587295862389E12,
     "user_tz": -180.0,
     "elapsed": 33932.0,
     "user": {
      "displayName": "Александр Тюрин",
      "photoUrl": "",
      "userId": "16220637881071339753"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1345/1345 [00:06<00:00, 204.38it/s]\n"
     ]
    }
   ],
   "source": [
    "h, t = a.generate_types_sets(p=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "tBPgNO2n92Co",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "class TransT(nn.Module):\n",
    "    \"\"\"\n",
    "    Наша модель, которая будет обучать ембеддинги. \n",
    "    Требует доработки\n",
    "    \"\"\"\n",
    "    def __init__(self, train_dataset, vector_length=200): \n",
    "        super(TransT, self).__init__()\n",
    "        self.train_dataset = train_dataset\n",
    "        self.vector_length = vector_length\n",
    "        self.entity_emb = self.generate_embedddings()\n",
    "        self.relation_emb = nn.Embedding(self.train_dataset.num_rel, self.vector_length)\n",
    "\n",
    "    def generate_embedddings(self):\n",
    "        ls = []\n",
    "        for i in tqdm(range(self.train_dataset.num_ent)):\n",
    "            ls.append(nn.Embedding(len(self.train_dataset.types_dict[i]), self.vector_length))\n",
    "\n",
    "        return ls \n",
    "\n",
    "    # def forward(self, pos_triplets, negative_triplets):\n",
    "    #     # This method will be written soon \n",
    "    #     print(pos_triplets.shape, neg_triplets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "jIwiZle892E-",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "outputId": "87e4b9a3-6fa9-46fa-8dc2-ff7f78bc2b8f",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.587295973439E12,
     "user_tz": -180.0,
     "elapsed": 2109.0,
     "user": {
      "displayName": "Александр Тюрин",
      "photoUrl": "",
      "userId": "16220637881071339753"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14951/14951 [00:01<00:00, 10474.94it/s]\n"
     ]
    }
   ],
   "source": [
    "model = TransT(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "DWkOhbbl92J-",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "TransT.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyOHWQktkJkWu/wl/+hamJl/"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
